{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import os\n",
    "import time\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "#from envs.burgers import Burgers\n",
    "from buffer import OfflineReplayBuffer\n",
    "from critic import ValueLearner, QPiLearner, QSarsaLearner\n",
    "from bppo import BehaviorCloning, BehaviorProximalPolicyOptimization\n",
    "from envs.burgers import Burgers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "# Experiment\n",
    "env_name='burger'\n",
    "path='logs'\n",
    "log_freq=int(100)\n",
    "seed=20241219\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "N=100 # Number of trajectories to collect for offline dataset\n",
    "\n",
    "# For Value\n",
    "v_steps=int(5000)\n",
    "v_hidden_dim = 256\n",
    "v_depth = 3\n",
    "v_lr = 1e-4\n",
    "v_batch_size = 64\n",
    "\n",
    "# For Q\n",
    "q_bc_steps=int(5000)\n",
    "q_pi_steps=10 # Number of steps to update Q-network in each iteration. Only used if is_offpolicy_update=True.\n",
    "q_hidden_dim = 256\n",
    "q_depth = 3\n",
    "q_lr = 1e-4\n",
    "q_batch_size = 64\n",
    "target_update_freq=2\n",
    "tau=0.005 # Soft update rate for target Q network parameters. See Q_learner.update()\n",
    "gamma=0.99 # Discount factor for calculating the return.\n",
    "is_offpolicy_update=False # Whether to use advantage replacement (as proposed in the BPPO paper) in Q-learning.\n",
    "# If False, use Q-learning to update the Q-network parameters in each iteration.\n",
    "# If True, only update the Q-network parameters once, and keep using this Q-network.\n",
    "\n",
    "# For BC\n",
    "bc_steps=int(500)\n",
    "bc_lr = 1e-4\n",
    "bc_hidden_dim = 256\n",
    "bc_depth = 3\n",
    "bc_batch_size = 64\n",
    "\n",
    "# For BPPO\n",
    "bppo_steps=int(100)\n",
    "bppo_hidden_dim = 256\n",
    "bppo_depth = 3\n",
    "bppo_lr = 1e-4\n",
    "bppo_batch_size = 64\n",
    "clip_ratio=0.25 # PPO clip ratio. The probability ratio between new and old policy is clipped to be in the range [1-clip_ratio, 1+clip_ratio]\n",
    "entropy_weight=0.00 # Weight of entropy loss in PPO and BPPO. Can be set to 0.01 for medium tasks.\n",
    "decay=0.96 # Decay rate of PPO clip ratio\n",
    "omega=0.9 # Related to setting the weight of advantage (see PPO code)\n",
    "is_clip_decay=True # Whether to decay the clip_ratio during training\n",
    "is_bppo_lr_decay=True # Whether to decay the learning rate of BPPO during trainining\n",
    "is_update_old_policy=True # Whether to update the old policy of BPPO in each iteration. The old policy is used to calculate the probability ratio.\n",
    "is_state_norm=False # Whether to normalize the states of the dataset.\n",
    "\n",
    "# Other Settings\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device=torch.device('cpu')\n",
    "state_dim = 128\n",
    "action_dim = 128\n",
    "x_range=(-5,5)\n",
    "nt=500\n",
    "dt=0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_burgers() got an unexpected keyword argument 'visualize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgenerate_burgers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_burgers\n\u001b[1;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_burgers(\n\u001b[0;32m      4\u001b[0m                 x_range\u001b[38;5;241m=\u001b[39mx_range,\n\u001b[0;32m      5\u001b[0m                 nt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m, \u001b[38;5;66;03m# Number of time steps\u001b[39;00m\n\u001b[0;32m      6\u001b[0m                 nx \u001b[38;5;241m=\u001b[39m state_dim, \u001b[38;5;66;03m# Number of spatial nodes (grid points)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m                 dt\u001b[38;5;241m=\u001b[39m dt, \u001b[38;5;66;03m# Temporal interval\u001b[39;00m\n\u001b[0;32m      8\u001b[0m                 N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;66;03m# Number of samples (trajectories) to generate\u001b[39;00m\n\u001b[0;32m      9\u001b[0m                 visualize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m# Whether to show the animation of state trajectory evolution\u001b[39;00m\n\u001b[0;32m     10\u001b[0m                 )\n\u001b[0;32m     11\u001b[0m target_state\u001b[38;5;241m=\u001b[39mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY_f\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\u001b[1;31mTypeError\u001b[0m: load_burgers() got an unexpected keyword argument 'visualize'"
     ]
    }
   ],
   "source": [
    "from generate_burgers import load_burgers\n",
    "\n",
    "dataset = load_burgers(\n",
    "                x_range=x_range,\n",
    "                nt = 500, # Number of time steps\n",
    "                nx = state_dim, # Number of spatial nodes (grid points)\n",
    "                dt= dt, # Temporal interval\n",
    "                N = 100, # Number of samples (trajectories) to generate\n",
    "                visualize=False # Whether to show the animation of state trajectory evolution\n",
    "                )\n",
    "target_state=dataset['Y_f']\n",
    "\n",
    "for key in dataset.keys():\n",
    "    if key!=\"meta_data\":\n",
    "        dataset[key]=dataset[key].squeeze(0)\n",
    "\n",
    "print(dataset['observations'].shape)\n",
    "print(dataset['actions'].shape)\n",
    "print(dataset['rewards'].shape)\n",
    "print(dataset['terminals'].shape)\n",
    "print(dataset['timeouts'].shape)\n",
    "print(dataset['Y_f'].shape)\n",
    "\n",
    "\n",
    "env=Burgers(x_range, nx=state_dim, nt=nt, dt=dt, energy_penalty=0.01, device='cpu')\n",
    "env.reset()\n",
    "env.set_target_state(target_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing the returns: 499it [00:00, 374880.48it/s]\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = OfflineReplayBuffer(device, state_dim, action_dim, len(dataset['actions']))\n",
    "replay_buffer.load_dataset(dataset=dataset)\n",
    "replay_buffer.compute_return(gamma) # Compute the discounted return for the trajectory, with a discount factor of gamma (default 0.99)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made log directory at logs\\20241219\\2024_12_22__14_14_26\n"
     ]
    }
   ],
   "source": [
    "# summarywriter logger\n",
    "# path\n",
    "\n",
    "current_time = time.strftime(\"%Y_%m_%d__%H_%M_%S\", time.localtime())\n",
    "path = os.path.join(path, str(seed))\n",
    "os.makedirs(os.path.join(path, current_time))\n",
    "print(f'Made log directory at {os.path.join(path, current_time)}')\n",
    "\n",
    "logger_path = os.path.join(path, current_time)\n",
    "logger = SummaryWriter(log_dir=logger_path, comment='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilize\n",
    "value = ValueLearner(device=device,\n",
    "                        state_dim=state_dim,\n",
    "                        hidden_dim=v_hidden_dim,\n",
    "                        depth=v_depth,\n",
    "                        value_lr=v_lr,\n",
    "                        batch_size=v_batch_size)\n",
    "\n",
    "Q_bc = QSarsaLearner(device=device,\n",
    "                        state_dim=state_dim,\n",
    "                        action_dim=action_dim,\n",
    "                        hidden_dim=q_hidden_dim, depth=q_depth,\n",
    "                        Q_lr=q_lr,\n",
    "                        target_update_freq=target_update_freq,\n",
    "                        tau=tau,\n",
    "                        gamma=gamma,\n",
    "                        batch_size=q_batch_size)\n",
    "if is_offpolicy_update: \n",
    "    Q_pi=QPiLearner(device=device,\n",
    "                        state_dim=state_dim,\n",
    "                        action_dim=action_dim,\n",
    "                        hidden_dim=q_hidden_dim,\n",
    "                        depth=q_depth,\n",
    "                        Q_lr=q_lr,\n",
    "                        target_update_freq=target_update_freq,\n",
    "                        tau=tau,\n",
    "                        gamma=gamma,\n",
    "                        batch_size=q_batch_size)\n",
    "bc=BehaviorCloning(device=device,\n",
    "                        state_dim=state_dim,\n",
    "                        hidden_dim=bc_hidden_dim,\n",
    "                        depth=bc_depth,\n",
    "                        action_dim=action_dim,\n",
    "                        policy_lr=bc_lr,\n",
    "                        batch_size=bc_batch_size)\n",
    "bppo=BehaviorProximalPolicyOptimization(device=device,\n",
    "                        state_dim=state_dim,\n",
    "                        hidden_dim=bppo_hidden_dim,\n",
    "                        depth=bppo_depth,\n",
    "                        action_dim=action_dim,\n",
    "                        policy_lr=bppo_lr,\n",
    "                        clip_ratio=clip_ratio,\n",
    "                        entropy_weight=entropy_weight,\n",
    "                        decay=decay,\n",
    "                        omega=omega,\n",
    "                        batch_size=bppo_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value parameters loaded\n",
      "Q function parameters loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\PythonProjects\\BPPO_custom\\critic.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self._value.load_state_dict(torch.load(path, map_location=self._device))\n",
      "e:\\PythonProjects\\BPPO_custom\\critic.py:147: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self._Q.load_state_dict(torch.load(path, map_location=self._device))\n"
     ]
    }
   ],
   "source": [
    "# value training \n",
    "value_path = os.path.join(path, 'value.pt')\n",
    "if os.path.exists(value_path):\n",
    "    value.load(value_path)\n",
    "else:\n",
    "    for step in tqdm.tqdm(range(int(v_steps)), desc='value updating ......'):\n",
    "        value_loss = value.update(replay_buffer)\n",
    "        \n",
    "        if step % int(log_freq) == 0:\n",
    "            print(f\"Step: {step}, Loss: {value_loss:.6f}\")\n",
    "            logger.add_scalar('value_loss', value_loss, global_step=(step+1))\n",
    "    value.save(value_path)\n",
    "\n",
    "# Q_bc training\n",
    "Q_bc_path = os.path.join(path, 'Q_bc.pt')\n",
    "if os.path.exists(Q_bc_path):\n",
    "    Q_bc.load(Q_bc_path)\n",
    "else:\n",
    "    for step in tqdm.tqdm(range(int(q_bc_steps)), desc='Q_bc updating ......'):\n",
    "        Q_bc_loss = Q_bc.update(replay_buffer, pi=None)\n",
    "        if step % int(log_freq) == 0:\n",
    "            print(f\"Step: {step}, Loss: {Q_bc_loss:.6f}\")\n",
    "            logger.add_scalar('Q_bc_loss', Q_bc_loss, global_step=(step+1))\n",
    "    Q_bc.save(Q_bc_path)\n",
    "\n",
    "if is_offpolicy_update:\n",
    "    Q_pi.load(Q_bc_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Behavior policy parameters loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\PythonProjects\\BPPO_custom\\bppo.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self._policy.load_state_dict(torch.load(path, map_location=self._device))\n"
     ]
    }
   ],
   "source": [
    "mean, std = 0., 1.\n",
    "\n",
    "# bc training\n",
    "best_bc_path = os.path.join(path, 'bc_best.pt')\n",
    "if os.path.exists(best_bc_path):\n",
    "    bc.load(best_bc_path)\n",
    "else:\n",
    "    best_bc_score = 0\n",
    "    for step in tqdm.tqdm(range(int(bc_steps)), desc='bc updating ......'):\n",
    "        bc_loss = bc.update(replay_buffer)\n",
    "        if step % int(log_freq) == 0:\n",
    "            current_bc_score = bc.offline_evaluate(env, seed)\n",
    "            if current_bc_score > best_bc_score:\n",
    "                best_bc_score = current_bc_score\n",
    "                bc.save(best_bc_path)\n",
    "                np.savetxt(os.path.join(path, 'best_bc.csv'), [best_bc_score], fmt='%f', delimiter=',')\n",
    "            print(f\"Step: {step}, Loss: {bc_loss:.4f}, Score: {current_bc_score:.4f}\")\n",
    "            logger.add_scalar('bc_loss', bc_loss, global_step=(step+1))\n",
    "            logger.add_scalar('bc_score', current_bc_score, global_step=(step+1))\n",
    "    bc.save(os.path.join(path, 'bc_best.pt'))\n",
    "    bc.load(best_bc_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\PythonProjects\\BPPO_custom\\ppo.py:193: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self._policy.load_state_dict(torch.load(path, map_location=self._device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy parameters loaded\n",
      "best_bppo_score: -10000.0 -------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bppo updating ......:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bppo updating ......:   1%|          | 1/100 [00:10<18:06, 10.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: -0.2860, Score: -10000.0000\n",
      "\n",
      "Epoch 2:\n",
      "Step: 1, Loss: 0.1025, Score: -10000.0000\n",
      "\n",
      "Epoch 3:\n",
      "Step: 2, Loss: 0.0300, Score: -10000.0000\n",
      "\n",
      "Epoch 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bppo updating ......:   9%|▉         | 9/100 [00:11<01:09,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_bppo_score: -9259.149132811173 -------------------------\n",
      "Policy parameters saved in logs\\20241219\\2024_12_22__14_14_26\\bppo_best.pt\n",
      "Step: 3, Loss: 0.0297, Score: -9259.1491\n",
      "\n",
      "Epoch 5:\n",
      "Step: 4, Loss: -0.3098, Score: -10000.0000\n",
      "\n",
      "Epoch 6:\n",
      "Step: 5, Loss: -0.0427, Score: -10000.0000\n",
      "\n",
      "Epoch 7:\n",
      "Step: 6, Loss: 0.0134, Score: -10000.0000\n",
      "\n",
      "Epoch 8:\n",
      "Step: 7, Loss: 0.0345, Score: -10000.0000\n",
      "\n",
      "Epoch 9:\n",
      "Step: 8, Loss: 0.0289, Score: -10000.0000\n",
      "\n",
      "Epoch 10:\n",
      "Step: 9, Loss: 0.0324, Score: -10000.0000\n",
      "\n",
      "Epoch 11:\n",
      "Step: 10, Loss: 0.0332, Score: -10000.0000\n",
      "\n",
      "Epoch 12:\n",
      "Step: 11, Loss: 0.0294, Score: -10000.0000\n",
      "\n",
      "Epoch 13:\n",
      "Step: 12, Loss: 0.0332, Score: -10000.0000\n",
      "\n",
      "Epoch 14:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bppo updating ......:  19%|█▉        | 19/100 [00:11<00:20,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 13, Loss: 0.0294, Score: -10000.0000\n",
      "\n",
      "Epoch 15:\n",
      "Step: 14, Loss: 0.0277, Score: -10000.0000\n",
      "\n",
      "Epoch 16:\n",
      "Step: 15, Loss: 0.0309, Score: -10000.0000\n",
      "\n",
      "Epoch 17:\n",
      "Step: 16, Loss: 0.0279, Score: -10000.0000\n",
      "\n",
      "Epoch 18:\n",
      "Step: 17, Loss: 0.0298, Score: -10000.0000\n",
      "\n",
      "Epoch 19:\n",
      "Step: 18, Loss: 0.0292, Score: -10000.0000\n",
      "\n",
      "Epoch 20:\n",
      "Step: 19, Loss: 0.0345, Score: -10000.0000\n",
      "\n",
      "Epoch 21:\n",
      "Step: 20, Loss: 0.0287, Score: -10000.0000\n",
      "\n",
      "Epoch 22:\n",
      "Step: 21, Loss: 0.0338, Score: -10000.0000\n",
      "\n",
      "Epoch 23:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bppo updating ......:  29%|██▉       | 29/100 [00:12<00:08,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 22, Loss: 0.0347, Score: -10000.0000\n",
      "\n",
      "Epoch 24:\n",
      "Step: 23, Loss: 0.0335, Score: -10000.0000\n",
      "\n",
      "Epoch 25:\n",
      "Step: 24, Loss: 0.0338, Score: -10000.0000\n",
      "\n",
      "Epoch 26:\n",
      "Step: 25, Loss: 0.0387, Score: -10000.0000\n",
      "\n",
      "Epoch 27:\n",
      "Step: 26, Loss: 0.0336, Score: -10000.0000\n",
      "\n",
      "Epoch 28:\n",
      "Step: 27, Loss: 0.0321, Score: -10000.0000\n",
      "\n",
      "Epoch 29:\n",
      "Step: 28, Loss: 0.0368, Score: -10000.0000\n",
      "\n",
      "Epoch 30:\n",
      "Step: 29, Loss: 0.0391, Score: -10000.0000\n",
      "\n",
      "Epoch 31:\n",
      "Step: 30, Loss: 0.0340, Score: -10000.0000\n",
      "\n",
      "Epoch 32:\n",
      "Step: 31, Loss: 0.0363, Score: -10000.0000\n",
      "\n",
      "Epoch 33:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bppo updating ......:  39%|███▉      | 39/100 [00:12<00:04, 15.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 32, Loss: 0.0350, Score: -10000.0000\n",
      "\n",
      "Epoch 34:\n",
      "Step: 33, Loss: 0.0346, Score: -10000.0000\n",
      "\n",
      "Epoch 35:\n",
      "Step: 34, Loss: 0.0316, Score: -10000.0000\n",
      "\n",
      "Epoch 36:\n",
      "Step: 35, Loss: 0.0340, Score: -10000.0000\n",
      "\n",
      "Epoch 37:\n",
      "Step: 36, Loss: 0.0339, Score: -10000.0000\n",
      "\n",
      "Epoch 38:\n",
      "Step: 37, Loss: 0.0378, Score: -10000.0000\n",
      "\n",
      "Epoch 39:\n",
      "Step: 38, Loss: 0.0325, Score: -10000.0000\n",
      "\n",
      "Epoch 40:\n",
      "Step: 39, Loss: 0.0292, Score: -10000.0000\n",
      "\n",
      "Epoch 41:\n",
      "Step: 40, Loss: 0.0375, Score: -10000.0000\n",
      "\n",
      "Epoch 42:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bppo updating ......:  49%|████▉     | 49/100 [00:12<00:02, 22.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 41, Loss: 0.0361, Score: -10000.0000\n",
      "\n",
      "Epoch 43:\n",
      "Step: 42, Loss: 0.0334, Score: -10000.0000\n",
      "\n",
      "Epoch 44:\n",
      "Step: 43, Loss: 0.0335, Score: -10000.0000\n",
      "\n",
      "Epoch 45:\n",
      "Step: 44, Loss: 0.0356, Score: -10000.0000\n",
      "\n",
      "Epoch 46:\n",
      "Step: 45, Loss: 0.0334, Score: -10000.0000\n",
      "\n",
      "Epoch 47:\n",
      "Step: 46, Loss: 0.0355, Score: -10000.0000\n",
      "\n",
      "Epoch 48:\n",
      "Step: 47, Loss: 0.0355, Score: -10000.0000\n",
      "\n",
      "Epoch 49:\n",
      "Step: 48, Loss: 0.0337, Score: -10000.0000\n",
      "\n",
      "Epoch 50:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bppo updating ......:  57%|█████▋    | 57/100 [00:12<00:01, 27.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 49, Loss: 0.0384, Score: -10000.0000\n",
      "\n",
      "Epoch 51:\n",
      "Step: 50, Loss: 0.0307, Score: -10000.0000\n",
      "\n",
      "Epoch 52:\n",
      "Step: 51, Loss: 0.0322, Score: -10000.0000\n",
      "\n",
      "Epoch 53:\n",
      "Step: 52, Loss: 0.0335, Score: -10000.0000\n",
      "\n",
      "Epoch 54:\n",
      "Step: 53, Loss: 0.0377, Score: -10000.0000\n",
      "\n",
      "Epoch 55:\n",
      "Step: 54, Loss: 0.0302, Score: -10000.0000\n",
      "\n",
      "Epoch 56:\n",
      "Step: 55, Loss: 0.0369, Score: -10000.0000\n",
      "\n",
      "Epoch 57:\n",
      "Step: 56, Loss: 0.0349, Score: -10000.0000\n",
      "\n",
      "Epoch 58:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bppo updating ......:  62%|██████▏   | 62/100 [00:12<00:01, 31.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 57, Loss: 0.0326, Score: -10000.0000\n",
      "\n",
      "Epoch 59:\n",
      "Step: 58, Loss: 0.0338, Score: -10000.0000\n",
      "\n",
      "Epoch 60:\n",
      "Step: 59, Loss: 0.0373, Score: -10000.0000\n",
      "\n",
      "Epoch 61:\n",
      "Step: 60, Loss: 0.0368, Score: -10000.0000\n",
      "\n",
      "Epoch 62:\n",
      "Step: 61, Loss: 0.0392, Score: -10000.0000\n",
      "\n",
      "Epoch 63:\n",
      "Step: 62, Loss: 0.0373, Score: -10000.0000\n",
      "\n",
      "Epoch 64:\n",
      "Step: 63, Loss: 0.0376, Score: -10000.0000\n",
      "\n",
      "Epoch 65:\n",
      "Step: 64, Loss: 0.0344, Score: -10000.0000\n",
      "\n",
      "Epoch 66:\n",
      "Step: 65, Loss: 0.0376, Score: -10000.0000\n",
      "\n",
      "Epoch 67:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bppo updating ......:  67%|██████▋   | 67/100 [00:12<00:00, 33.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 66, Loss: 0.0311, Score: -10000.0000\n",
      "\n",
      "Epoch 68:\n",
      "Step: 67, Loss: 0.0364, Score: -10000.0000\n",
      "\n",
      "Epoch 69:\n",
      "Step: 68, Loss: 0.0343, Score: -10000.0000\n",
      "\n",
      "Epoch 70:\n",
      "Step: 69, Loss: 0.0328, Score: -10000.0000\n",
      "\n",
      "Epoch 71:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bppo updating ......:  72%|███████▏  | 72/100 [00:13<00:01, 18.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_bppo_score: -9092.083142119134 -------------------------\n",
      "Policy parameters saved in logs\\20241219\\2024_12_22__14_14_26\\bppo_best.pt\n",
      "Step: 70, Loss: 0.0356, Score: -9092.0831\n",
      "\n",
      "Epoch 72:\n",
      "Step: 71, Loss: -0.2931, Score: -10000.0000\n",
      "\n",
      "Epoch 73:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bppo updating ......:  76%|███████▌  | 76/100 [00:14<00:01, 13.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 72, Loss: -0.1428, Score: -9820.3164\n",
      "\n",
      "Epoch 74:\n",
      "Step: 73, Loss: -0.1199, Score: -10000.0000\n",
      "\n",
      "Epoch 75:\n",
      "Step: 74, Loss: -0.0802, Score: -10000.0000\n",
      "\n",
      "Epoch 76:\n",
      "Step: 75, Loss: -0.0392, Score: -10000.0000\n",
      "\n",
      "Epoch 77:\n",
      "Step: 76, Loss: -0.1682, Score: -10000.0000\n",
      "\n",
      "Epoch 78:\n",
      "Step: 77, Loss: -0.0383, Score: -10000.0000\n",
      "\n",
      "Epoch 79:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bppo updating ......:  84%|████████▍ | 84/100 [00:14<00:01, 14.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 78, Loss: 0.0443, Score: -9674.4577\n",
      "\n",
      "Epoch 80:\n",
      "Step: 79, Loss: 0.0426, Score: -10000.0000\n",
      "\n",
      "Epoch 81:\n",
      "Step: 80, Loss: 0.0158, Score: -10000.0000\n",
      "\n",
      "Epoch 82:\n",
      "Step: 81, Loss: 0.0518, Score: -10000.0000\n",
      "\n",
      "Epoch 83:\n",
      "Step: 82, Loss: 0.0067, Score: -10000.0000\n",
      "\n",
      "Epoch 84:\n",
      "Step: 83, Loss: 0.0219, Score: -10000.0000\n",
      "\n",
      "Epoch 85:\n",
      "Step: 84, Loss: 0.0297, Score: -10000.0000\n",
      "\n",
      "Epoch 86:\n",
      "Step: 85, Loss: 0.0943, Score: -10000.0000\n",
      "\n",
      "Epoch 87:\n",
      "Step: 86, Loss: 0.0349, Score: -10000.0000\n",
      "\n",
      "Epoch 88:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bppo updating ......:  94%|█████████▍| 94/100 [00:14<00:00, 23.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 87, Loss: 0.2578, Score: -10000.0000\n",
      "\n",
      "Epoch 89:\n",
      "Step: 88, Loss: 0.0350, Score: -10000.0000\n",
      "\n",
      "Epoch 90:\n",
      "Step: 89, Loss: 0.0676, Score: -10000.0000\n",
      "\n",
      "Epoch 91:\n",
      "Step: 90, Loss: 0.0375, Score: -10000.0000\n",
      "\n",
      "Epoch 92:\n",
      "Step: 91, Loss: 0.0623, Score: -10000.0000\n",
      "\n",
      "Epoch 93:\n",
      "Step: 92, Loss: 0.0343, Score: -10000.0000\n",
      "\n",
      "Epoch 94:\n",
      "Step: 93, Loss: 0.0378, Score: -10000.0000\n",
      "\n",
      "Epoch 95:\n",
      "Step: 94, Loss: 0.0367, Score: -10000.0000\n",
      "\n",
      "Epoch 96:\n",
      "Step: 95, Loss: 0.0367, Score: -10000.0000\n",
      "\n",
      "Epoch 97:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bppo updating ......: 100%|██████████| 100/100 [00:14<00:00,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 96, Loss: 0.0363, Score: -10000.0000\n",
      "\n",
      "Epoch 98:\n",
      "Step: 97, Loss: 0.0359, Score: -10000.0000\n",
      "\n",
      "Epoch 99:\n",
      "Step: 98, Loss: 0.0381, Score: -10000.0000\n",
      "\n",
      "Epoch 100:\n",
      "Step: 99, Loss: 0.0388, Score: -10000.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# bppo training\n",
    "bppo.load(best_bc_path)\n",
    "best_bppo_path = os.path.join(path, current_time, 'bppo_best.pt')\n",
    "Q = Q_bc # If advantage replacement, then Q_{\\pi k}=Q_{\\pi\\beta}\n",
    "best_bppo_score = bppo.offline_evaluate(env, seed, eval_episodes=10)\n",
    "print('best_bppo_score:',best_bppo_score,'-------------------------')\n",
    "for step in tqdm.tqdm(range(int(bppo_steps)), desc='bppo updating ......'):\n",
    "    print(f\"\\nEpoch {step+1}:\")\n",
    "    if step > 200:\n",
    "        is_clip_decay = False\n",
    "        is_bppo_lr_decay = False\n",
    "    bppo_loss = bppo.update(replay_buffer, Q, value, is_clip_decay, is_bppo_lr_decay)\n",
    "    current_bppo_score = bppo.offline_evaluate(env, seed, eval_episodes=10) # J_{\\pi k}\n",
    "    if current_bppo_score > best_bppo_score:\n",
    "        best_bppo_score = current_bppo_score\n",
    "        print('best_bppo_score:',best_bppo_score,'-------------------------')\n",
    "        bppo.save(best_bppo_path)\n",
    "        np.savetxt(os.path.join(path, current_time, 'best_bppo.csv'), [best_bppo_score], fmt='%f', delimiter=',')\n",
    "        if is_update_old_policy:\n",
    "            bppo.set_old_policy() # Set the old policy to the current policy\n",
    "    if is_offpolicy_update: # If not using advantage replacement, calculate Q_{\\pi k} by Q-learning\n",
    "        for _ in tqdm(range(int(q_pi_steps)), desc='Q_pi updating ......'): \n",
    "            Q_pi_loss = Q_pi.update(replay_buffer, bppo)\n",
    "        Q = Q_pi\n",
    "    print(f\"Step: {step}, Loss: {bppo_loss:.4f}, Score: {current_bppo_score:.4f}\")\n",
    "    logger.add_scalar('bppo_loss', bppo_loss, global_step=(step+1))\n",
    "    logger.add_scalar('bppo_score', current_bppo_score, global_step=(step+1))\n",
    "\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Double and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(timesteps):\n\u001b[0;32m     11\u001b[0m     env\u001b[38;5;241m.\u001b[39mreset(y0)\n\u001b[1;32m---> 12\u001b[0m     action\u001b[38;5;241m=\u001b[39mbppo\u001b[38;5;241m.\u001b[39m_policy(torch\u001b[38;5;241m.\u001b[39mtensor(y))\u001b[38;5;241m.\u001b[39msample()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     13\u001b[0m     env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(((target_state\u001b[38;5;241m-\u001b[39my)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean())\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\env_py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\env_py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\PythonProjects\\BPPO_custom\\net.py:106\u001b[0m, in \u001b[0;36mGaussPolicyMLP.forward\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m, s: torch\u001b[38;5;241m.\u001b[39mTensor\n\u001b[0;32m    104\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions:\n\u001b[1;32m--> 106\u001b[0m     mu, log_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_net(s)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoft_clamp\u001b[39m(x: torch\u001b[38;5;241m.\u001b[39mTensor, bound: \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    108\u001b[0m         low, high \u001b[38;5;241m=\u001b[39m bound\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\env_py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\env_py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\env_py312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\env_py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\env_py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\env_py312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Double and Float"
     ]
    }
   ],
   "source": [
    "from generate_burgers import generate_initial_y\n",
    "\n",
    "x=np.linspace(*x_range, state_dim)\n",
    "y0=generate_initial_y(x)\n",
    "#y0=torch.tensor(y0, dtype=torch.float32)\n",
    "\n",
    "timesteps=16\n",
    "y=y0\n",
    "\n",
    "for _ in range(timesteps):\n",
    "    env.reset(y0)\n",
    "    action=bppo._policy(torch.tensor(y)).sample().detach().numpy()\n",
    "    env.step(action)\n",
    "\n",
    "\n",
    "\n",
    "print(((target_state-y)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
