{
    "Results": {
        "BPPO": {
            "bppo_target_loss": 0.0036345999588220917,
            "bppo_energy;": 4.160891850224291e-05
        },
        "BC": {
            "bc_target_loss": 0.0036342355597214883,
            "bc_energy": 4.199347596259032e-05
        },
        "Natural Evolution": {
            "natural_target_loss": 0.0038127237953255893,
            "natural_energy": 0.0
        }
    },
    "Experiment Settings": {
        "dataset_name": "ip",
        "seed": 20250112,
        "train_data_path": "datasets\\inverted_pendulum_99900_127_2_1.pkl",
        "test_data_path": "datasets\\inverted_pendulum_100_127_2_1.pkl",
        "result_dir": "logs",
        "model_save_dir": "logs\\ip\\event_1736937816",
        "log_dir": "logs\\ip\\event_1736937816",
        "fig_save_dir": "logs\\ip\\event_1736937816\\figs",
        "log_freq": 1000
    },
    "Value Network Settings": {
        "v_steps": 100000,
        "v_hidden_dim": 512,
        "v_depth": 3,
        "v_lr": 0.0001,
        "v_batch_size": 64
    },
    "Q Network Settings": {
        "q_bc_steps": 100000,
        "q_pi_steps": 10,
        "q_hidden_dim": 512,
        "q_depth": 3,
        "q_lr": 0.0001,
        "q_batch_size": 64,
        "target_update_freq": 2,
        "tau": 0.005,
        "gamma": 0.99
    },
    "BC Settings": {
        "bc_steps": 100000,
        "bc_lr": 0.0001,
        "bc_hidden_dim": 512,
        "bc_depth": 3,
        "bc_batch_size": 64
    },
    "BPPO Settings": {
        "bppo_steps": 20000,
        "bppo_hidden_dim": 512,
        "bppo_depth": 3,
        "bppo_lr": 0.0001,
        "bppo_batch_size": 64,
        "clip_ratio": 0.25,
        "entropy_weight": 0.0,
        "decay": 0.96,
        "omega": 0.9,
        "is_clip_decay": true,
        "is_bppo_lr_decay": true,
        "is_update_old_policy": true
    },
    "Others": {
        "N": 99900,
        "state_dim": 2,
        "action_dim": 1,
        "nt": 127,
        "action_reward_scale": 10.0,
        "average state reward": -7.827105054092381,
        "average action reward": -0.8335042914478861
    }
}