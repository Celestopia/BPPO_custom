{
    "Results": {
        "BPPO": {
            "bppo_target_loss": 0.029356027969167413,
            "bppo_energy;": 8.800170257768515e-06
        },
        "BC": {
            "bc_target_loss": 0.028174909647909127,
            "bc_energy": 8.719437156128151e-06
        },
        "Natural Evolution": {
            "natural_target_loss": 0.054163413536692365,
            "natural_energy": 0.0
        }
    },
    "Experiment Settings": {
        "dataset_name": "power",
        "seed": 20250112,
        "train_data_path": "datasets\\power_99900_31_18_9.pkl",
        "test_data_path": "datasets\\power_100_31_18_9.pkl",
        "result_dir": "logs",
        "model_save_dir": "logs\\power\\event_1736939147",
        "log_dir": "logs\\power\\event_1736939147",
        "fig_save_dir": "logs\\power\\event_1736939147\\figs",
        "log_freq": 1000
    },
    "Value Network Settings": {
        "v_steps": 100000,
        "v_hidden_dim": 512,
        "v_depth": 3,
        "v_lr": 0.0001,
        "v_batch_size": 64
    },
    "Q Network Settings": {
        "q_bc_steps": 100000,
        "q_pi_steps": 10,
        "q_hidden_dim": 512,
        "q_depth": 3,
        "q_lr": 0.0001,
        "q_batch_size": 64,
        "target_update_freq": 2,
        "tau": 0.005,
        "gamma": 0.99
    },
    "BC Settings": {
        "bc_steps": 100000,
        "bc_lr": 0.0001,
        "bc_hidden_dim": 512,
        "bc_depth": 3,
        "bc_batch_size": 64
    },
    "BPPO Settings": {
        "bppo_steps": 20000,
        "bppo_hidden_dim": 512,
        "bppo_depth": 3,
        "bppo_lr": 0.0001,
        "bppo_batch_size": 64,
        "clip_ratio": 0.25,
        "entropy_weight": 0.0,
        "decay": 0.96,
        "omega": 0.9,
        "is_clip_decay": true,
        "is_bppo_lr_decay": true,
        "is_update_old_policy": true
    },
    "Others": {
        "N": 99900,
        "state_dim": 18,
        "action_dim": 9,
        "nt": 31,
        "action_reward_scale": 1000.0,
        "average state reward": -1.5605556372290785,
        "average action reward": -0.8998178286900633
    }
}